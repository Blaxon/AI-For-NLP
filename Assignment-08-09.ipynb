{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key points\n",
    "+ 机器学习的基本概念\n",
    "+ 数据分布、数据预处理\n",
    "+ Linear Regression, Logstic Regression\n",
    "+ Gredient Descent\n",
    "+ 作业练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础概念复习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 机器学习方法主要用在什么特点的常见下？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "1. 拥有大量数据，人工无法处理的情况下   \n",
    "2. 数据复杂度高，但是数据格式统一   \n",
    "3. 可以用分类或者回归的方法解决当前问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 提出 3 个你认为使用了机器学习方法的现实场景."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "1. 垃圾邮件处理   \n",
    "2. 购物商品推荐   \n",
    "3. 天气预报"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 提出 3 个你认为可以使用机器学习但是还没有使用机器学习方法的场景. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "1. 软件测试   \n",
    "2. 项目管理   \n",
    "3. 剧本创作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 什么是“模型”？ 为什么说“All models are wrong, but some useful”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "a. 模型是对于现实生活中某个事件或者问题的抽象   \n",
    "b. 这句话的意思是说没有什么模型是可以完全等效于实际现象的，但是部分模型对目前我们解决问题是有效的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Classification 和 Regressionu主要针对什么？ 有什么区别？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "\n",
    "分类和回归的主要区别是分类是对样本进行静态的划分，但是回归是归纳样本的动态趋势   \n",
    "一个有时间维度，一个没有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. precision， recall，f1, auc 分别是什么意思？ 假设一个城市有 10000 人，有 30 个犯罪分子，警察抓到了 35 个人，其中 20 个是犯罪分子，请问这个警察的 precision, recall, f1,auc 分别是什么？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Precision（准确率）是预测的数据中真正正确的数据比率，Recall（召回率）是预测中正确的数据占实际数据的比率。   \n",
    "P = TP/(TP+FP)    P = 20/35   \n",
    "R = TP/(TP+FN)    R = 20/30   \n",
    "F1 = 2*P*R/(P+R)   F1 = 2*4/7*2/3/(4/7+2/3)   \n",
    "AUC 是ROC曲线下的面积，通过对不同分类方法算出AUC可以比较其方法的好坏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 请提出两种场景，第一种场景下，对模型的评估很注重 precision, 第二种很注重 recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "precision: 股票预测，量化交易，可以漏但不能判断失误   \n",
    "recall: 罪犯判断，癌症判断，可以错判（通过人工筛查矫正），但是不能漏判"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 什么是 Overfitting， 什么是 Underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "过拟合是说模型适配训练集数据太高，失去了泛化能力，或受噪音数据影响过于严重   \n",
    "欠拟合是说模型不够精准，缺失了对于某些关键特征判断，无法有效解决相应问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Lazy-Learning， Lazy在哪里？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "Lazy Learning的意思是该学习方法只有到了输入具体问题的时候才会根据当前样本数据进行计算分析，而不是提前准备好模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Median， Mode， Mean分别是什么？ 有什么意义？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Median中位数，表示将样本按大小排列，中间的数值即是中位数   \n",
    "Mode众数，表示样本中出现次数最多的数据   \n",
    "Mean平均数，顾名思义   \n",
    "通过了解这三个数据，我们可以分析出样本的分部形态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Outlinear（异常值、离群值）是什么？ 如何定义？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "离群点（outlier）是指数值中，远离数值的一般水平的极端大值和极端小值。 因此，也称之为歧异值，有时也称其为野值。 形成离群点的主要原因有：首先可能是采样中的误差，如记录的偏误，工作人员出现笔误，计算错误等，都有可能产生极端大值或者极端小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Bias 和 Variance 有什么关系？ 他们之间为什么是一种 tradeoff 的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.   \n",
    "Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.   \n",
    "https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Train， Validation，Test 数据集之间是什么关系？ 为什么要这么划分？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "Training set 指模型是由该数据生成的；   \n",
    "Validation set 指在机器学习中，模型是使用该数据集进行校正的（bias 和 variance）；   \n",
    "Test set 指最终验证模型好坏的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Supervised Learning 的 Supervised 体现在什么地方？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:    \n",
    "监督学习的监督表现在数据样本是进行标记过，有正确答案的，计算机会根据正确答案来进行学习；   \n",
    "而非监督学习的数据是没有标记过，没有正确答案的，机器需要根据现有的数据特征来自行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Linear Regression 中，什么是“线性关系”？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "因变量Y对于未知回归系数b1,b2,b3...是线性的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Linear Regression中，Loss 函数怎么定义的？ 为什么要写成这样？ 什么是凸函数？ 优化中有什么意义？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "Mean square error = 1/n * sum((yi - f(xi))\\**2)   \n",
    "错的越多惩罚越大   \n",
    "凸函数的定义是： f(a*x1 + (1-a)*x2) <= a*f(x1) + (1-a)*f(x2) 其中 0<=a<=1   \n",
    "凸函数便于使用梯度下降的方法找到最优loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. 简述Gradient Descent的过程，以 $y = -10 * x^2 + 3x + 4 $ 为例，从一个任一点 $ x = 10 $ 开始，如果根据 Gradient Descent 找到最值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-966\n",
      "-197\n"
     ]
    }
   ],
   "source": [
    "# Ans: \n",
    "def f(x): return -10*x**2 + 3*x + 4  # func is a concave function, so we need to find its max\n",
    "def fd(x): return -20*x + 3\n",
    "print(f(10))  # -996\n",
    "print(fd(10))  # -197 so x should move negative step, lets say step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-779\n",
      "-177\n"
     ]
    }
   ],
   "source": [
    "print(f(10-1))\n",
    "print(fd(10-1))  # continue until x = 3/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. 一般在机器学习数量时，会做一个预处理（Normalization）， 简述 Normalization 的过程，以及数据经过 Normalization之后的平均值和标准差的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029\n",
    "a good explain.   \n",
    "简单来说，normalization是把不同的特征统一到一个数值区间，这样可以减少不同特征由于数据大小范围差异大导致的学习效果不好的问题。\n",
    "https://blog.csdn.net/qq_28618765/article/details/78221571   \n",
    "Z-score nomralization后的均值为0，标准差为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Logstic Regression 的 Logstic 是什么曲线，被用在什么地方？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "f(x)=1/(1+e**-x)  \n",
    "它主要被用来处理二分类问题   \n",
    "https://baike.baidu.com/item/logistic%E5%9B%9E%E5%BD%92#2   \n",
    "https://en.wikipedia.org/wiki/Logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Logstic Regression 的 Loss 函数 Cross Entropy 是怎么样的形式？ 有什么意义？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans   \n",
    "关于交叉熵的说明：https://blog.csdn.net/rtygbwwwerr/article/details/50778098   \n",
    "https://blog.csdn.net/tsyccnh/article/details/79163834   \n",
    "   \n",
    "关于Log loss的说明：https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html   \n",
    "\n",
    "Log Loss\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/ng_cost_function_logistic.png\">\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/logistic_cost_function_joined.png\">\n",
    "\n",
    "意义在于其loss函数是平滑且单调的，这对于计算梯度和最小化Loss非常有帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题描述： 在新闻出版业中一个常常的问题就是新闻版权抄袭，所以我们现在为了避免这个事情，需要建立一个模型，判断这个文章是不是由某个新闻出版单位出版的。 在我们这个问题里，我们需要建立一个模型，该模型接受一个作为文本的输入，然后判断该文本是不是由“新华社”发布的。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enviroment: \n",
    "\n",
    "+ Python 3.6\n",
    "+ numpy \n",
    "+ scikit learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**请在 pycharm 中运行程序，该处只作为关键信息的记录。 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 问什么此问题应该用机器学习方法？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans：   \n",
    "1. 我们有足够的数据   \n",
    "2. 人工分类时间长成本高   \n",
    "3. 固定的分类规则很容易被破解   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 问什么要对文本进行向量化？ 如何进行文本向量化表示？ （请使用tfidf 或者词向量）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "向量化能够是文章能够使用机器学习的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: 如果你使用 tfidf，则需要 scikit learning 如果你需要词向量，则需要 gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 请对数据进行Preprocessing, Normalization 操作\n",
    "（你需要在 Preprocssing 的时候，把文章开头的“新华社”3 个字去掉。如果不去掉，会出现什么问题？）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans： 会导致模型失效，‘新华社’词语权重极大，无法准确预测抄袭文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 请确定模型的 Baseline 以及确定评测指标（Evaluation）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans：F1可以作为评测指标，它同时考虑了准确度和召回率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 尝试不同的模型、不同的参数，观察结果变化。 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TFIDF max feature = 1000时   \n",
    "\n",
    "accuracy =  0.9619206248923095\n",
    "f1 score(macro)= 0.8757333403263312\n",
    "f1 score(micro)= 0.9619206248923095\n",
    "f1 score(binary)= 0.979223465262762\n",
    "\n",
    "=================================\n",
    "TFIDF max feature = 10000时\n",
    "\n",
    "accuracy =  0.9666302911952214\n",
    "f1 score(macro)= 0.8900720622078058\n",
    "f1 score(micro)= 0.9666302911952214\n",
    "f1 score(binary)= 0.9818102125794433\n",
    "\n",
    "=================================\n",
    "TFIDF max feature = 100000时\n",
    "\n",
    "accuracy =  0.9615760151628281\n",
    "f1 score(macro)= 0.8682405349444062\n",
    "f1 score(micro)= 0.9615760151628281\n",
    "f1 score(binary)= 0.9791361297364729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 依据模型的表现，进行参数调节。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/hang.xiang/Downloads/shipping_promotion_rule_mass_upload_template.csv', 'r') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/hang.xiang/Downloads/shipping_promotion_rule_mass_upload_template_400.csv', 'w') as f:\n",
    "    f.writelines(content[:2])\n",
    "    f.writelines([content[2]]*301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,\n"
     ]
    }
   ],
   "source": [
    "t = ''\n",
    "for i in range(690,990):\n",
    "    t += '%d,'%i\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba, re, numpy\n",
    "\n",
    "csvfile = '/Users/hang.xiang/Desktop/sqlResult_1558435.csv'\n",
    "content = pd.read_csv(csvfile, delimiter=\",\", quotechar='\"', escapechar='\\\\')\n",
    "content.fillna('')\n",
    "source_n_content = content[['content','source']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(s): return ' '.join(jieba.cut(s))\n",
    "def token(s): return re.findall(r'[\\d|\\w]+', s)\n",
    "# remove 新华社 in Content\n",
    "rawdata_X = [cut(' '.join(token(news.replace('新华社', '')))) for news in source_n_content['content']]\n",
    "rawdata_Y = [1 if source == \"新华社\" else 0 for source in source_n_content['source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(rawdata_X, \n",
    "                                                                rawdata_Y, \n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=7)\n",
    "x_all = TfidfVectorizer(max_features=100000).fit(rawdata_X)\n",
    "x_train_vec = x_all.transform(x_train)\n",
    "x_validation_vec = x_all.transform(x_validation)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9615760151628281\n",
      "f1 score(macro)= 0.8682405349444062\n",
      "f1 score(micro)= 0.9615760151628281\n",
      "f1 score(binary)= 0.9791361297364729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = lr.predict(x_validation_vec)\n",
    "\n",
    "print('accuracy = ', lr.score(x_validation_vec, y_validation))\n",
    "print('f1 score(macro)=', f1_score(y_validation, y_pred, average='macro'))\n",
    "print('f1 score(micro)=', f1_score(y_validation, y_pred, average='micro'))\n",
    "print('f1 score(binary)=', f1_score(y_validation, y_pred, average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
